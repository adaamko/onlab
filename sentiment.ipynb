{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adaamko/onlab/blob/master/sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb6aygy812Az",
        "colab_type": "code",
        "outputId": "58a6f7db-c23b-43dc-a6b4-e01bfb3bc913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allennlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/8c/72b14d20c9cbb0306939ea41109fc599302634fd5c59ccba1a659b7d0360/allennlp-0.8.4-py3-none-any.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Collecting unidecode (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/57/2f0a46538295b8e7f09625da6dd24c23f9d0d7ef119ca1c33528660130d5/tensorboardX-1.7-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.2,>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting awscli>=1.11.91 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/f7/4a2ed465345a87d20cb6c3fc6bec33f1a4d18974223b88711300a0ff1d4a/awscli-1.16.184-py2.py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 33.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Collecting conllu==0.11 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Collecting jsonpickle (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/a8/adba6cd0f84ee6ab064e7f70cd03a2836cefd2e063fd565180ec13beae93/jsonnet-0.13.0.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.3)\n",
            "Collecting flaky (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.21.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.4)\n",
            "Collecting flask-cors>=3.0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Collecting numpydoc>=0.8.0 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/f3/7cfe4c616e4b9fe05540256cc9c6661c052c8a4cec2915732793b36e1843/numpydoc-0.9.1.tar.gz\n",
            "Collecting word2number>=1.1 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Collecting ftfy (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/86/df789c5834f15ae1ca53a8d4c1fc4788676c2e32112f6a786f2625d9c6e6/ftfy-5.5.1-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 21.0MB/s \n",
            "\u001b[?25hCollecting overrides (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting parsimonious>=0.8.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 20.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Collecting responses>=0.7 (from allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\n",
            "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.167)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (1.12.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (7.0.4)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.9.6)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.6.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.2.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (0.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0.18->allennlp) (2.0.2)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.2.1)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Collecting botocore==1.12.174 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/74/fcca5254234a55517d1016d8e0e342a53a642ff3216d156416abeedef3ea/botocore-1.12.174-py2.py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 38.6MB/s \n",
            "\u001b[?25hCollecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n",
            "  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n",
            "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<=5.1,>=3.10; python_version != \"2.6\" in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.15.4)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.10.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.13.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.0.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.0.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Collecting regex (from pytorch-pretrained-bert>=0.6.0->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->flask>=1.0.2->allennlp) (1.1.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.7.0)\n",
            "Building wheels for collected packages: jsonnet, numpydoc, word2number, overrides, parsimonious, regex\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/30/ab/ae4a57b1df44fa20a531edb9601b27603da8f5336225691f3f\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/30/d1/92a39ba40f21cb70e53f8af96eb98f002a781843c065406500\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built jsonnet numpydoc word2number overrides parsimonious regex\n",
            "Installing collected packages: unidecode, tensorboardX, botocore, colorama, rsa, awscli, conllu, jsonpickle, jsonnet, flaky, flask-cors, numpydoc, word2number, ftfy, overrides, parsimonious, responses, regex, pytorch-pretrained-bert, allennlp\n",
            "  Found existing installation: botocore 1.12.167\n",
            "    Uninstalling botocore-1.12.167:\n",
            "      Successfully uninstalled botocore-1.12.167\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "Successfully installed allennlp-0.8.4 awscli-1.16.184 botocore-1.12.174 colorama-0.3.9 conllu-0.11 flaky-3.5.3 flask-cors-3.0.8 ftfy-5.5.1 jsonnet-0.13.0 jsonpickle-1.2 numpydoc-0.9.1 overrides-1.9 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 regex-2019.6.8 responses-0.10.6 rsa-3.4.2 tensorboardX-1.7 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49jMMynx8ulc",
        "colab_type": "code",
        "outputId": "3a7fb7fc-4883-4c3c-f403-66400862a095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
        "    StanfordSentimentTreeBankDatasetReader\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
        "from allennlp.training.trainer import Trainer\n",
        "\n",
        "from allennlp.common import JsonDict\n",
        "from allennlp.data import DatasetReader, Instance\n",
        "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
        "from allennlp.predictors import Predictor\n",
        "from overrides import overrides"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ7bJ66M8VTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@Predictor.register(\"sentence_classifier_predictor\")\n",
        "class SentenceClassifierPredictor(Predictor):\n",
        "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
        "        super().__init__(model, dataset_reader)\n",
        "        self._tokenizer = SpacyWordSplitter(language='en_core_web_sm', pos_tags=True)\n",
        "\n",
        "    def predict(self, sentence: str) -> JsonDict:\n",
        "        return self.predict_json({\"sentence\" : sentence})\n",
        "\n",
        "    @overrides\n",
        "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
        "        sentence = json_dict[\"sentence\"]\n",
        "        tokens = self._tokenizer.split_words(sentence)\n",
        "        return self._dataset_reader.text_to_instance([str(t) for t in tokens])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Inq07rl1gg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFLVXrMq6q4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model in AllenNLP represents a model that is trained.\n",
        "@Model.register(\"lstm_classifier\")\n",
        "class LstmClassifier(Model):\n",
        "    def __init__(self,\n",
        "                 word_embeddings: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 vocab: Vocabulary,\n",
        "                 positive_label: str = '4') -> None:\n",
        "        super().__init__(vocab)\n",
        "        # We need the embeddings to convert word IDs to their vector representations\n",
        "        self.word_embeddings = word_embeddings\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        # After converting a sequence of vectors to a single vector, we feed it into\n",
        "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
        "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "\n",
        "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
        "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
        "        self.accuracy = CategoricalAccuracy()\n",
        "        self.f1_measure = F1Measure(positive_index)\n",
        "\n",
        "        # We use the cross entropy loss because this is a classification task.\n",
        "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
        "        # which makes it unnecessary to add a separate softmax layer.\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Instances are fed to forward after batching.\n",
        "    # Fields are passed through arguments with the same name.\n",
        "    def forward(self,\n",
        "                tokens: Dict[str, torch.Tensor],\n",
        "                label: torch.Tensor = None) -> torch.Tensor:\n",
        "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
        "        # shorter sequences get padded with zeros to make them equal length.\n",
        "        # Masking is the process to ignore extra zeros added by padding\n",
        "        mask = get_text_field_mask(tokens)\n",
        "\n",
        "        # Forward pass\n",
        "        embeddings = self.word_embeddings(tokens)\n",
        "        encoder_out = self.encoder(embeddings, mask)\n",
        "        logits = self.linear(encoder_out)\n",
        "\n",
        "        # In AllenNLP, the output of forward() is a dictionary.\n",
        "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
        "        output = {\"logits\": logits}\n",
        "        if label is not None:\n",
        "            self.accuracy(logits, label)\n",
        "            self.f1_measure(logits, label)\n",
        "            output[\"loss\"] = self.loss_function(logits, label)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        precision, recall, f1_measure = self.f1_measure.get_metric(reset)\n",
        "        return {'accuracy': self.accuracy.get_metric(reset),\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1_measure': f1_measure}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VOfyKAO-L8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    reader = StanfordSentimentTreeBankDatasetReader()\n",
        "\n",
        "    train_dataset = reader.read('drive/My Drive/data/trees/train.txt')\n",
        "    dev_dataset = reader.read('drive/My Drive/data/trees/dev.txt')\n",
        "\n",
        "    # You can optionally specify the minimum count of tokens/labels.\n",
        "    # `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
        "    # will be ignored and not included in the vocabulary.\n",
        "    vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
        "                                      min_count={'tokens': 3})\n",
        "\n",
        "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                                embedding_dim=EMBEDDING_DIM)\n",
        "\n",
        "    # BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
        "    # not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
        "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "\n",
        "    # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
        "    # (usually a sequence of embedded word vectors), processes it, and returns a single\n",
        "    # vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
        "    # AllenNLP also supports CNNs and other simple architectures (for example,\n",
        "    # just averaging over the input vectors).\n",
        "    encoder = PytorchSeq2VecWrapper(\n",
        "        torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
        "\n",
        "    model = LstmClassifier(word_embeddings, encoder, vocab)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "\n",
        "    iterator.index_with(vocab)\n",
        "\n",
        "    trainer = Trainer(model=model,\n",
        "                      optimizer=optimizer,\n",
        "                      iterator=iterator,\n",
        "                      train_dataset=train_dataset,\n",
        "                      validation_dataset=dev_dataset,\n",
        "                      patience=10,\n",
        "                      num_epochs=20)\n",
        "    trainer.train()\n",
        "\n",
        "    predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "    logits = predictor.predict('This is the best movie ever!')['logits']\n",
        "    label_id = np.argmax(logits)\n",
        "\n",
        "    print(model.vocab.get_token_from_index(label_id, 'labels'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXCVwYMC-iMN",
        "colab_type": "code",
        "outputId": "781a9b39-d1b7-4d2d-c4af-60393916452c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqlZrWZk-0u5",
        "colab_type": "code",
        "outputId": "975f4d27-6af2-4be2-dc2e-9fe7d74cff02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "reader = StanfordSentimentTreeBankDatasetReader()\n",
        "\n",
        "train_dataset = reader.read('drive/My Drive/data/trees/train.txt')\n",
        "dev_dataset = reader.read('drive/My Drive/data/trees/dev.txt')\n",
        "\n",
        "# You can optionally specify the minimum count of tokens/labels.\n",
        "# `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
        "# will be ignored and not included in the vocabulary.\n",
        "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
        "                                  min_count={'tokens': 3})\n",
        "\n",
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "\n",
        "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
        "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "\n",
        "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
        "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
        "# vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
        "# AllenNLP also supports CNNs and other simple architectures (for example,\n",
        "# just averaging over the input vectors).\n",
        "encoder = PytorchSeq2VecWrapper(\n",
        "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
        "\n",
        "model = LstmClassifier(word_embeddings, encoder, vocab)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "\n",
        "iterator.index_with(vocab)\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=dev_dataset,\n",
        "                  patience=10,\n",
        "                  num_epochs=20)\n",
        "trainer.train()\n",
        "\n",
        "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "logits = predictor.predict('This is the best movie ever!')['logits']\n",
        "label_id = np.argmax(logits)\n",
        "\n",
        "print(model.vocab.get_token_from_index(label_id, 'labels'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8544it [00:02, 3260.20it/s]\n",
            "1101it [00:00, 2419.39it/s]\n",
            "100%|██████████| 9645/9645 [00:00<00:00, 45613.08it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.2649, precision: 0.1565, recall: 0.0458, f1_measure: 0.0709, loss: 1.5823 ||: 100%|██████████| 267/267 [00:12<00:00, 20.75it/s]\n",
            "accuracy: 0.2534, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5730 ||: 100%|██████████| 35/35 [00:00<00:00, 74.81it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.2718, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5652 ||: 100%|██████████| 267/267 [00:12<00:00, 21.99it/s]\n",
            "accuracy: 0.2534, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5734 ||: 100%|██████████| 35/35 [00:00<00:00, 89.17it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.2786, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5540 ||: 100%|██████████| 267/267 [00:11<00:00, 22.33it/s]\n",
            "accuracy: 0.2906, precision: 0.0000, recall: 0.0000, f1_measure: 0.0000, loss: 1.5768 ||: 100%|██████████| 35/35 [00:00<00:00, 89.27it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.3074, precision: 0.6277, recall: 0.0668, f1_measure: 0.1207, loss: 1.5177 ||: 100%|██████████| 267/267 [00:12<00:00, 22.14it/s]\n",
            "accuracy: 0.3061, precision: 0.5250, recall: 0.1273, f1_measure: 0.2049, loss: 1.5446 ||: 100%|██████████| 35/35 [00:00<00:00, 92.84it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.3645, precision: 0.4909, recall: 0.5419, f1_measure: 0.5151, loss: 1.4265 ||: 100%|██████████| 267/267 [00:11<00:00, 21.94it/s]\n",
            "accuracy: 0.3206, precision: 0.3879, recall: 0.2727, f1_measure: 0.3203, loss: 1.5063 ||: 100%|██████████| 35/35 [00:00<00:00, 90.28it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.4232, precision: 0.5670, recall: 0.6995, f1_measure: 0.6263, loss: 1.3351 ||: 100%|██████████| 267/267 [00:11<00:00, 22.49it/s]\n",
            "accuracy: 0.3324, precision: 0.4100, recall: 0.2485, f1_measure: 0.3094, loss: 1.5180 ||: 100%|██████████| 35/35 [00:00<00:00, 92.13it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.4676, precision: 0.6412, recall: 0.7438, f1_measure: 0.6887, loss: 1.2431 ||: 100%|██████████| 267/267 [00:11<00:00, 22.56it/s]\n",
            "accuracy: 0.3424, precision: 0.3732, recall: 0.4727, f1_measure: 0.4171, loss: 1.5300 ||: 100%|██████████| 35/35 [00:00<00:00, 89.73it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.5060, precision: 0.6775, recall: 0.7896, f1_measure: 0.7293, loss: 1.1577 ||: 100%|██████████| 267/267 [00:11<00:00, 22.81it/s]\n",
            "accuracy: 0.3379, precision: 0.3771, recall: 0.4000, f1_measure: 0.3882, loss: 1.5908 ||: 100%|██████████| 35/35 [00:00<00:00, 89.44it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.5373, precision: 0.7303, recall: 0.8222, f1_measure: 0.7736, loss: 1.0797 ||: 100%|██████████| 267/267 [00:11<00:00, 22.34it/s]\n",
            "accuracy: 0.3542, precision: 0.3974, recall: 0.3636, f1_measure: 0.3797, loss: 1.6217 ||: 100%|██████████| 35/35 [00:00<00:00, 90.45it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.5750, precision: 0.7628, recall: 0.8463, f1_measure: 0.8024, loss: 1.0057 ||: 100%|██████████| 267/267 [00:11<00:00, 22.27it/s]\n",
            "accuracy: 0.3560, precision: 0.3695, recall: 0.4545, f1_measure: 0.4076, loss: 1.6604 ||: 100%|██████████| 35/35 [00:00<00:00, 90.42it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.6140, precision: 0.7973, recall: 0.8579, f1_measure: 0.8265, loss: 0.9348 ||: 100%|██████████| 267/267 [00:11<00:00, 18.79it/s]\n",
            "accuracy: 0.3479, precision: 0.3627, recall: 0.4485, f1_measure: 0.4011, loss: 1.7919 ||: 100%|██████████| 35/35 [00:00<00:00, 92.55it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.6403, precision: 0.8102, recall: 0.8781, f1_measure: 0.8428, loss: 0.8743 ||: 100%|██████████| 267/267 [00:11<00:00, 24.18it/s]\n",
            "accuracy: 0.3460, precision: 0.3556, recall: 0.3879, f1_measure: 0.3710, loss: 1.8430 ||: 100%|██████████| 35/35 [00:00<00:00, 91.99it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.6715, precision: 0.8329, recall: 0.8936, f1_measure: 0.8622, loss: 0.8151 ||: 100%|██████████| 267/267 [00:12<00:00, 24.44it/s]\n",
            "accuracy: 0.3506, precision: 0.3571, recall: 0.3636, f1_measure: 0.3604, loss: 2.0128 ||: 100%|██████████| 35/35 [00:00<00:00, 92.79it/s]\n",
            "unable to check gpu_memory_mb(), continuing\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/allennlp/common/util.py\", line 378, in gpu_memory_mb\n",
            "    encoding='utf-8')\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 356, in check_output\n",
            "    **kwargs).stdout\n",
            "  File \"/usr/lib/python3.6/subprocess.py\", line 438, in run\n",
            "    output=stdout, stderr=stderr)\n",
            "subprocess.CalledProcessError: Command '['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']' returned non-zero exit status 9.\n",
            "accuracy: 0.6995, precision: 0.8696, recall: 0.9119, f1_measure: 0.8903, loss: 0.7500 ||:  42%|████▏     | 112/267 [00:05<00:06, 23.46it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSGVdnwBwB1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = predictor.predict('great!')['logits']\n",
        "label_id = np.argmax(logits)\n",
        "\n",
        "print(model.vocab.get_token_from_index(label_id, 'labels'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEUfrXK8GyBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}